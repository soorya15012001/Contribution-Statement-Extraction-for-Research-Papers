{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libpoppler-cpp-dev\n",
        "!pip install pdftotext\n",
        "!pip install pdfminer.six\n",
        "!pip install -U textblob\n",
        "!pip install multi_rake\n",
        "!pip install rouge-score\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxtedZgrb-8o",
        "outputId": "c125d1e8-2901-4562-e18e-4550bcea0f71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpoppler-cpp0v5\n",
            "The following NEW packages will be installed:\n",
            "  libpoppler-cpp-dev libpoppler-cpp0v5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 36.7 kB of archives.\n",
            "After this operation, 188 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpoppler-cpp0v5 amd64 0.62.0-2ubuntu2.12 [28.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpoppler-cpp-dev amd64 0.62.0-2ubuntu2.12 [8,676 B]\n",
            "Fetched 36.7 kB in 0s (90.1 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpoppler-cpp0v5:amd64.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../libpoppler-cpp0v5_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking libpoppler-cpp0v5:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Selecting previously unselected package libpoppler-cpp-dev:amd64.\n",
            "Preparing to unpack .../libpoppler-cpp-dev_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking libpoppler-cpp-dev:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Setting up libpoppler-cpp0v5:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Setting up libpoppler-cpp-dev:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pdftotext\n",
            "  Downloading pdftotext-2.2.2.tar.gz (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdftotext\n",
            "  Building wheel for pdftotext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdftotext: filename=pdftotext-2.2.2-cp37-cp37m-linux_x86_64.whl size=54914 sha256=043270e2b78c4f7cfe6576ac2d7ee0fa4bec07cd7e5b1db6e4d8ae08f16d97e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/19/8e/e8648026db8b7ef3324ad9afa1f7c9109a7e7509846f693ed9\n",
            "Successfully built pdftotext\n",
            "Installing collected packages: pdftotext\n",
            "Successfully installed pdftotext-2.2.2\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 37.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-36.0.2 pdfminer.six-20220319\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Installing collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.15.3\n",
            "    Uninstalling textblob-0.15.3:\n",
            "      Successfully uninstalled textblob-0.15.3\n",
            "Successfully installed textblob-0.17.1\n",
            "Collecting multi_rake\n",
            "  Downloading multi_rake-0.0.2-py3-none-any.whl (31 kB)\n",
            "Collecting pycld2>=0.41\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.4 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (1.21.5)\n",
            "Requirement already satisfied: pyrsistent>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (0.18.1)\n",
            "Requirement already satisfied: regex>=2018.6.6 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (2019.12.20)\n",
            "Building wheels for collected packages: pycld2\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834164 sha256=5a442331b18870b70b1c2149378370991541d455cf718d46ea596f0170bb50b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n",
            "Successfully built pycld2\n",
            "Installing collected packages: pycld2, multi-rake\n",
            "Successfully installed multi-rake-0.0.2 pycld2-0.41\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir GloVe\n",
        "!curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
        "\n",
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFDoGqIohc3t",
        "outputId": "765747dd-ddfc-4ed9-e617-ca3ba98a78e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0   352    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  5145k      0  0:06:53  0:06:53 --:--:-- 5565k\n",
            "Archive:  GloVe/glove.840B.300d.zip\n",
            "  inflating: GloVe/glove.840B.300d.txt  \n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  18.4M      0  0:00:07  0:00:07 --:--:-- 25.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Facebook infersent official helper class\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from multi_rake import Rake\n",
        "\n",
        "\n",
        "\n",
        "class InferSent(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(InferSent, self).__init__()\n",
        "        self.bsize = config['bsize']\n",
        "        self.word_emb_dim = config['word_emb_dim']\n",
        "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
        "        self.pool_type = config['pool_type']\n",
        "        self.dpout_model = config['dpout_model']\n",
        "        self.version = 1 if 'version' not in config else config['version']\n",
        "\n",
        "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
        "                                bidirectional=True, dropout=self.dpout_model)\n",
        "\n",
        "        assert self.version in [1, 2]\n",
        "        if self.version == 1:\n",
        "            self.bos = '<s>'\n",
        "            self.eos = '</s>'\n",
        "            self.max_pad = True\n",
        "            self.moses_tok = False\n",
        "        elif self.version == 2:\n",
        "            self.bos = '<p>'\n",
        "            self.eos = '</p>'\n",
        "            self.max_pad = False\n",
        "            self.moses_tok = True\n",
        "\n",
        "    def is_cuda(self):\n",
        "        # either all weights are on cpu or they are on gpu\n",
        "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
        "\n",
        "    def forward(self, sent_tuple):\n",
        "        # sent_len: [max_len, ..., min_len] (bsize)\n",
        "        # sent: (seqlen x bsize x worddim)\n",
        "        sent, sent_len = sent_tuple\n",
        "\n",
        "        # Sort by length (keep idx)\n",
        "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
        "        sent_len_sorted = sent_len_sorted.copy()\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_sort)\n",
        "        sent = sent.index_select(1, idx_sort)\n",
        "\n",
        "        # Handling padding in Recurrent Networks\n",
        "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
        "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
        "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
        "\n",
        "        # Un-sort by length\n",
        "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_unsort)\n",
        "        sent_output = sent_output.index_select(1, idx_unsort)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pool_type == \"mean\":\n",
        "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
        "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
        "            emb = emb / sent_len.expand_as(emb)\n",
        "        elif self.pool_type == \"max\":\n",
        "            if not self.max_pad:\n",
        "                sent_output[sent_output == 0] = -1e9\n",
        "            emb = torch.max(sent_output, 0)[0]\n",
        "            if emb.ndimension() == 3:\n",
        "                emb = emb.squeeze(0)\n",
        "                assert emb.ndimension() == 2\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def set_w2v_path(self, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "\n",
        "    def get_word_dict(self, sentences, tokenize=True):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.bos] = ''\n",
        "        word_dict[self.eos] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in word_dict:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "    def get_w2v_k(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with k first w2v vectors\n",
        "        k = 0\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if k <= K:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    k += 1\n",
        "                if k > K:\n",
        "                    if word in [self.bos, self.eos]:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "\n",
        "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
        "                    break\n",
        "        return word_vec\n",
        "\n",
        "    def build_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "        self.word_vec = self.get_w2v(word_dict)\n",
        "        print('Vocab size : %s' % (len(self.word_vec)))\n",
        "\n",
        "    # build w2v vocab with k most frequent words\n",
        "    def build_vocab_k_words(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        self.word_vec = self.get_w2v_k(K)\n",
        "        print('Vocab size : %s' % (K))\n",
        "\n",
        "    def update_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
        "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "\n",
        "        # keep only new words\n",
        "        for word in self.word_vec:\n",
        "            if word in word_dict:\n",
        "                del word_dict[word]\n",
        "\n",
        "        # udpate vocabulary\n",
        "        if word_dict:\n",
        "            new_word_vec = self.get_w2v(word_dict)\n",
        "            self.word_vec.update(new_word_vec)\n",
        "        else:\n",
        "            new_word_vec = []\n",
        "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
        "\n",
        "    def get_batch(self, batch):\n",
        "        # sent in batch in decreasing order of lengths\n",
        "        # batch: (bsize, max_len, word_dim)\n",
        "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(len(batch[i])):\n",
        "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
        "\n",
        "        return torch.FloatTensor(embed)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        if self.moses_tok:\n",
        "            s = ' '.join(word_tokenize(s))\n",
        "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
        "            return s.split()\n",
        "        else:\n",
        "            return word_tokenize(s)\n",
        "\n",
        "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
        "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
        "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
        "        n_w = np.sum([len(x) for x in sentences])\n",
        "\n",
        "        # filters words without w2v vectors\n",
        "        for i in range(len(sentences)):\n",
        "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
        "            if not s_f:\n",
        "                import warnings\n",
        "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
        "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
        "                s_f = [self.eos]\n",
        "            sentences[i] = s_f\n",
        "\n",
        "        lengths = np.array([len(s) for s in sentences])\n",
        "        n_wk = np.sum(lengths)\n",
        "        if verbose:\n",
        "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
        "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
        "\n",
        "        # sort by decreasing length\n",
        "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
        "        sentences = np.array(sentences)[idx_sort]\n",
        "\n",
        "        return sentences, lengths, idx_sort\n",
        "\n",
        "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
        "        tic = time.time()\n",
        "        sentences, lengths, idx_sort = self.prepare_samples(\n",
        "                        sentences, bsize, tokenize, verbose)\n",
        "\n",
        "        embeddings = []\n",
        "        for stidx in range(0, len(sentences), bsize):\n",
        "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
        "            if self.is_cuda():\n",
        "                batch = batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
        "            embeddings.append(batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # unsort\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "        embeddings = embeddings[idx_unsort]\n",
        "\n",
        "        if verbose:\n",
        "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
        "                    len(embeddings)/(time.time()-tic),\n",
        "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
        "        return embeddings\n",
        "\n",
        "    def visualize(self, sent, tokenize=True):\n",
        "\n",
        "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
        "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
        "\n",
        "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
        "            import warnings\n",
        "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
        "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
        "        batch = self.get_batch(sent)\n",
        "\n",
        "        if self.is_cuda():\n",
        "            batch = batch.cuda()\n",
        "        output = self.enc_lstm(batch)[0]\n",
        "        output, idxs = torch.max(output, 0)\n",
        "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
        "        idxs = idxs.data.cpu().numpy()\n",
        "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
        "\n",
        "        # visualize model\n",
        "        import matplotlib.pyplot as plt\n",
        "        x = range(len(sent[0]))\n",
        "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
        "        plt.xticks(x, sent[0], rotation=45)\n",
        "        plt.bar(x, y)\n",
        "        plt.ylabel('%')\n",
        "        plt.title('Visualisation of words importance')\n",
        "        plt.show()\n",
        "\n",
        "        return output, idxs"
      ],
      "metadata": {
        "id": "CVCBJoffozSa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean(s):\n",
        "    lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = set(nltk.corpus.words.words())\n",
        "    stopwords_dict = Counter(stop_words)\n",
        "\n",
        "    s = re.sub(r\"http\\S+\", '', s)\n",
        "    s = re.sub('\\n', ' ', s)\n",
        "    s = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", s)\n",
        "    s = re.sub(r'.*?(?=Abstract)','',s,1)\n",
        "    s = re.sub(\"Conclusion.*\", \"\", s)\n",
        "    s = s.replace('- ', '')\n",
        "    s = s.replace('– ', ' ')\n",
        "    s = re.sub(\"\\s\\s+\" , \" \", s)\n",
        "    s = ' '.join([word for word in s.split() if word not in stopwords_dict])\n",
        "    s = \" \".join([lemmatizer.lemmatize(w) for w in s.split(' ')]).strip()\n",
        "    s = \" \".join(w for w in nltk.wordpunct_tokenize(s) if w.lower() in words or not w.isalpha() or len(w.lower()) > 2)\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34pfeuyHex9Q",
        "outputId": "92db5711-4975-4bf1-e393-0e26439a8810"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from io import StringIO\n",
        "\n",
        "def convert_pdf_to_txt(path):\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    codec = 'utf-8'\n",
        "    laparams = LAParams()\n",
        "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
        "    fp = open(path, 'rb')\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    password = \"\"\n",
        "    maxpages = 0\n",
        "    caching = True\n",
        "    pagenos=set()\n",
        "\n",
        "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "\n",
        "    text = retstr.getvalue()\n",
        "\n",
        "    fp.close()\n",
        "    device.close()\n",
        "    retstr.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "GLe0ykNMAcXY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = convert_pdf_to_txt(\"data.pdf\")\n",
        "text_str = clean(x)\n",
        "print(text_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SmYTrraCYG6",
        "outputId": "afe0c292-2b6c-4d53-c0ab-47d02f30ced6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract In paper , propose novel neural network model called RNN Encoder Decoder consists two recurrent neural network . One RNN encodes sequence symbol ﬁxedlength vector representation , decodes representation another sequence symbols . The encoder decoder proposed model jointly trained maximize conditional probability target sequence given source sequence . The performance statistical machine translation system empirically found improve using conditional probability phrase pair computed RNN Encoder – Decoder additional feature existing log - linear model . Qualitatively , show proposed model learns semantically syntactically meaningful representation linguistic phrases . 1 Introduction Deep neural network shown great success various application objection recognition ) speech recognition ). Furthermore , many recent work showed neural network successfully used number task natural language processing . These include , limited to , language modeling , paraphrase detection word embedding extraction . In ﬁeld statistical machine translation , deep neural network begun show promising results . summarizes successful usage feedforward neural network framework phrase - based SMT system . Along line research using neural network SMT , paper focus novel neural network architecture used part conventional phrase - based SMT system . The proposed neural network architecture , refer RNN Encoder – Decoder , consists two recurrent neural network act encoder decoder pair . The encoder map variable - length source sequence ﬁxed - length vector , decoder map vector representation back variable - length target sequence . The two network trained jointly maximize conditional probability target sequence given source sequence . Additionally , propose use rather sophisticated hidden unit order improve memory capacity ease training . The proposed RNN Encoder – Decoder novel hidden unit empirically evaluated task translating English French . We train model learn translation probability English phrase corresponding French phrase . The model used part standard phrase - based SMT system scoring phrase pair phrase table . The empirical evaluation reveals approach scoring phrase pair RNN Encoder – Decoder improves translation performance . We qualitatively analyze trained RNN Encoder – Decoder comparing phrase score given existing translation model . The qualitative analysis show RNN Encoder – Decoder better capturing linguistic regularity phrase table , indirectly explaining quantitative improvement overall translation performance . The analysis model reveals RNN Encoder Decoder learns continuous space representation phrase preserve semantic syntactic structure phrase . 4 1 0 2 p e S 3 ] L C . c neural network consists hidden state h optional output operates variablelength sequence x = . At time step t , hidden state RNN updated = f − 1 , , non - linear f activation funcf may simple elementtion . wise logistic sigmoid function complex long short - term memory unit . An RNN learn probability distribution sequence trained predict next symbol sequence . In case , output timestep conditional distribution p . For example , multinomial distribution output using softmax activation function p = K exp wjht j = 1 exp wjht possible symbol j = 1 , . . . , K , row weight matrix W . By combining probabilities , compute probability sequence x using Figure 1 : An illustration proposed RNN Encoder – Decoder . note input output sequence length T T may differ . The encoder RNN read symbol input sequence x sequentially . As read symbol , hidden state RNN change according . . After reading end sequence , hidden state RNN summary c whole input sequence . The decoder proposed model another RNN trained generate output sequence predicting next symbol given hidden state . However , unlike RNN described Sec . 2 . 1 , also conditioned − 1 summary c input sequence . Hence , hidden state decoder time computed by , , p = T t = 1 p . = f − 1 , − 1 , c , From learned distribution , straightforward sample new sequence iteratively sampling symbol time step . 2 . 2 RNN Encoder – Decoder In paper , propose novel neural network architecture learns encode variable - length sequence ﬁxed - length vector representation decode given ﬁxed - length vector representation back variable - length sequence . From probabilistic perspective , new model general method learn conditional distribution variable - length sequence conditioned yet another variable - length sequence , | x1 , . . . , ), one e . g . p similarly , conditional distribution next symbol P = g , − 1 , c . given activation function f g . See Fig . 1 graphical depiction pro posed model architecture . The two component proposed RNN Encoder – Decoder jointly trained maximize conditional log - likelihood max 1 N N n = 1 log , x1x2xTyT ' y2y1cDecoderEncoder set model parameter pair training set . In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters . Once RNN Encoder – Decoder trained , model used two ways . One way use model generate target sequence given input sequence . On hand , model used score given pair input output sequences , score simply probability Eqs . . 2 . 3 Hidden Unit Adaptively Remembers Forgets In addition novel model architecture , also propose new type hidden unit ) motivated LSTM unit much simpler compute implement . 1 Fig . 2 show graphical depiction proposed hidden unit . Let u describe activation j - th hidden unit computed . First , reset gate computed = j + Urht − 1 , j logistic sigmoid function , j denotes j - th element vector . x − 1 input previous hidden state , respectively . Ur weight matrix learned . Similarly , update gate computed = j + Uzht − 1 . j The actual activation proposed unit computed j = zjht − 1 j + ˜ j , ˜ j = j + U r − 1 . j In formulation , reset gate close 0 , hidden state forced ignore previous hidden state reset current input 1 The LSTM unit , shown impressive result several application speech recognition , memory cell four gating unit adaptively control information ﬂow inside unit , compared two gating unit proposed hidden unit . For detail LSTM networks , see , e . g ., . Figure 2 : An illustration proposed hidden activation function . The update gate z selects whether hidden state updated new hidden state ˜ h . The reset gate r decides whether previous hidden state ignored . See Eqs . detailed equation r , z , h ˜ h . only . This effectively allows hidden state drop information found irrelevant later future , thus , allowing compact representation . On hand , update gate control much information previous hidden state carry current hidden state . This act similarly memory cell LSTM network help RNN remember longterm information . Furthermore , may considered adaptive variant leaky - integration unit . As hidden unit separate reset update gates , hidden unit learn capture dependency different time scales . Those unit learn capture short - term dependency tend reset gate frequently active , capture longer - term dependency update gate mostly active . In preliminary experiments , found crucial use new unit gating units . We able get meaningful result oft - used tanh unit without gating . 3 Statistical Machine Translation In commonly used statistical machine translation system , goal system ﬁnd translation f given source sentence e , maximizes p ∝ , ﬁrst term right hand side called translation model latter language model In practice , however , ). SMT system model log p loglinear model additional feature corre zrhh ~ x sponding weights : log p = N n = 1 wnfn + log Z , In phrase - based n - th feature weight , respectively . Z normalization constant depend weights . The weight often optimized maximize BLEU score development set . framework introduced translation model , log p factorized translation probability matching phrase source target sentences . 2 These probability considered additional feature log - linear model ) weighted accordingly maximize BLEU score . SMT Since neural net language model proposed , neural network used widely SMT systems . In many cases , neural network used rescore translation hypothesis ). Recently , however , interest training neural network score translated sentence using representation source sentence additional input . See , e . g ., , . 3 . 1 Scoring Phrase Pairs RNN Encoder – Decoder Here propose train RNN Encoder Decoder table phrase pair use score additional feature loglinear model . tuning SMT decoder . When train RNN Encoder – Decoder , ignore frequency phrase pair original corpora . This measure taken order reduce computational expense randomly selecting phrase pair large phrase table according normalized frequency ensure RNN Encoder Decoder simply learn rank phrase pair according number occurrences . One underlying reason choice existing translation probability phrase table already reﬂects frequency phrase 2 Without loss generality , on , refer p phrase pair translation model well pair original corpus . With ﬁxed capacity RNN Encoder – Decoder , try ensure capacity model focused toward learning linguistic regularities , i . e ., distinguishing plausible implausible translations , learning “ manifold ” plausible translations . Once RNN Encoder – Decoder trained , add new score phrase pair existing phrase table . This allows new score enter existing tuning algorithm minimal additional overhead computation . As Schwenk pointed , possible completely replace existing phrase table proposed RNN Encoder Decoder . In case , given source phrase , RNN Encoder – Decoder need generate list target phrases . This requires , however , expensive sampling procedure perIn paper , thus , formed repeatedly . consider rescoring phrase pair phrase table . 3 . 2 Related Approaches : Neural Networks Machine Translation Before presenting empirical results , discus number recent work proposed use neural network context SMT . Schwenk proposed similar approach scoring phrase pairs . Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output . When used speciﬁcally scoring phrase SMT system , maximum phrase length often chosen small . However , length phrase increase apply neural network variable - length sequence data , important neural network handle variable - length input output . The proposed RNN Encoder – Decoder well - suited applications . Similar , Devlin al . proposed use feedforward neural network model translation model , however , predicting one word target phrase time . They reported impressive improvement , approach still requires maximum length input phrase ﬁxed priori . Although exactly neural network train , author proposed learn bilingual embedding words / phrases . They use learned embedding compute distance pair phrase used additional score phrase pair SMT system . In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase . This closely related proposed RNN Encoder – Decoder model proposed , except input representation phrase bag - of - words . A similar approach using bag - of - words representation proposed well . Earlier , similar encoder – decoder model using two recursive neural network proposed , model restricted monolingual setting , i . e . model reconstructs input sentence . More recently , another encoder – decoder model using RNN proposed , decoder conditioned representation either source sentence source context . One important difference proposed RNN Encoder – Decoder approach order word source target phrase taken account . The RNN Encoder – Decoder naturally distinguishes sequence word different order , whereas aforementioned approach effectively ignore order information . The closest approach related proposed RNN Encoder – Decoder Recurrent Continuous Translation Model proposed In pa . per , proposed similar model consists encoder decoder . The difference model used convolutional n - gram model encoder hybrid inverse CGM recurrent neural network decoder . They , however , evaluated model rescoring n - best list proposed conventional SMT system computing perplexity gold standard translations . 4 Experiments We evaluate approach English / French translation task WMT ’ 14 workshop . 4 . 1 Data Baseline System Large amount resource available build English / French SMT system framework WMT ’ 14 translation task . The bilingual corpus include Europarl , news commentary , UN , two crawled corpus 90M 780M word respectively . The last two corpus quite noisy . To train French language model , 712M word crawled newspaper material available addition target side bitexts . All word count refer French word tokenization . It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle . Instead , one focus relevant subset data given task . We done applying data selection method proposed , extension bitexts . By mean selected subset 418M word 2G word language modeling subset 348M 850M word training RNN Encoder – Decoder . We used test set newstest2012 2013 data selection weight tuning MERT , newstest2014 test set . Each set 70 thousand word single reference translation . For training neural networks , including proposed RNN Encoder – Decoder , limited source target vocabulary frequent 15 , 000 word English French . This cover approximately 93 % dataset . All out - of - vocabulary word mapped special token ). The baseline phrase - based SMT system built using Moses default settings . This system achieves BLEU score 30 . 64 33 . 3 development test sets , respectively . 4 . 1 . 1 RNN Encoder – Decoder The RNN Encoder – Decoder used experiment 1000 hidden unit proposed gate encoder decoder . The input matrix input symbol hidden unit approximated two lower - rank matrices , output matrix approximated BLEU tem add redundant . Models dev 30 . 64 Baseline 31 . 20 RNN 31 . 48 CSLM + RNN CSLM + RNN + 31 . 50 test 33 . 30 33 . 87 34 . 64 34 . 54 Table 1 : BLEU score computed development test set using different combination approaches . denotes word penalty , penalizes number unknown word neural networks . similarly . We used rank - 100 matrices , equivalent learning embedding dimension 100 word . The activation function used ˜ h . hyperbolic tangent function . The computation hidden state decoder output implemented deep neural network single intermediate layer 500 maxout unit pooling 2 input . All weight parameter RNN Encoder Decoder initialized sampling isotropic zero - mean Gaussian distribution standard deviation ﬁxed 0 . 01 , except recurrent weight parameters . For recurrent weight matrices , ﬁrst sampled white Gaussian distribution used left singular vector matrix , following . We used Adadelta stochastic gradient descent train RNN Encoder – Decoder hyperparameters = 10 − 6 = 0 . 95 . At update , used 64 randomly selected phrase pair phrase table . The model trained approximately three days . Details architecture used experiment explained depth supplementary material . 4 . 1 . 2 Neural Language Model In order ass effectiveness scoring phrase pair proposed RNN Encoder Decoder , also tried traditional approach using neural network learning target language model . Especially , comparison SMT system using CSLM using proposed approach phrase scoring RNN Encoder – Decoder clarify whether contribution multiple neural network different part SMT sys We trained CSLM model 7 - grams target corpus . Each input word projected embedding space R512 , concatenated form 3072dimensional vector . The concatenated vector fed two rectiﬁed layer . The output layer simple softmax layer ). All weight parameter initialized uniformly − 0 . 01 0 . 01 , model trained validation perplexity improve 10 epochs . After training , language model achieved perplexity 45 . 80 . The validation set random selection 0 . 1 % corpus . The model used score partial translation decoding process , generally lead higher gain BLEU score n - best list rescoring . To address computational complexity using CSLM decoder buffer used aggregate n - grams stacksearch performed decoder . Only full , stack buffer pruned , n - grams scored CSLM . This allows u perform fast matrixmatrix multiplication GPU using Theano . Figure 3 : The visualization phrase pair according score RNN Encoder – Decoder translation model . 4 . 2 Quantitative Analysis We tried following combinations : 1 . Baseline conﬁguration 2 . Baseline + RNN 3 . Baseline + CSLM + RNN 4 . Baseline + CSLM + RNN + Word penalty − 60 − 50 − 40 − 30 − 20 − 100 − 14 − 12 − 10 − 8 − 6 − 4 − 20RNN Scores Scores Source end ﬁrst time United States , well one Translation Model pour la premir ¨ ere fois ] RNN Encoder – Decoder Long , frequent source phrase Source , Minister Communications Transport comply part world . past day . Friday Saturday Translation Model gions monde .] RNN Encoder – Decoder Long , rare source phrase Table 2 : The top scoring target phrase small set source phrase according translation model RNN Encoder – Decoder . Source phrase randomly selected phrase 4 words . ? denotes incomplete character . r Cyrillic letter ghe . The result presented Table 1 . As expected , adding feature computed neural network consistently improves performance baseline performance . The best performance achieved used CSLM phrase score RNN Encoder – Decoder . This suggests contribution CSLM RNN Encoder Decoder correlated one expect better result improving method independently . Furthermore , tried penalizing number word unknown neural network . We simply adding number unknown word additional feature loglinear model . . 3 However , case 3 To understand effect penalty , consider set word 15 , 000 large shortlist , . All word xi /∈ replaced special token scored neural networks . Hence , conditional probability xi /∈ actually given model p | x < t ) = p = p | x < t ≥ p xi | x < t , /∈ x < t shorthand notation − 1 , . . . , x1 . able achieve better performance test set , development set . 4 . 3 Qualitative Analysis In order understand performance improvement come from , analyze phrase pair score computed RNN Encoder – Decoder corresponding p translation model . Since existing translation model relies solely statistic phrase pair corpus , expect score better estimated frequent phrase badly estimated rare phrases . Also , mentioned earlier Sec . 3 . 1 , expect RNN Encoder Decoder trained without frequency information score phrase pair based rather linguistic regularity statistic occurrence corpus . We focus pair whose source phrase long As result , probability word shortlist always overestimated . It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation . Source end ﬁrst time United States , well one Samples RNN Encoder – Decoder Long , frequent source phrase Source , Minister Communications Transport comply part world . past day . Friday Saturday Samples RNN Encoder – Decoder Long , rare source phrase Table 3 : Samples generated RNN Encoder – Decoder source phrase used Table 2 . We show top - 5 target phrase 50 samples . They sorted RNN Encoder – Decoder scores . Figure 4 : 2 – D embedding learned word representation . The left one show full embedding space , right one show zoomed - in view one region . For plots , see supplementary material . frequent . For source phrase , look target phrase scored high either translation probability p RNN Encoder – Decoder . Similarly , perform procedure pair whose source phrase long rare corpus . Table 2 list top - 3 target phrase per source phrase favored either translation model RNN Encoder – Decoder . The source phrase randomly chosen among long one 4 5 words . In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations . We observe RNN Encoder – Decoder prefers shorter phrase general . Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different . This could arise proposed approach training RNN Encoder Decoder set unique phrase pairs , discouraging RNN Encoder – Decoder learning simply frequency phrase pair corpus , explained earlier . Furthermore , Table 3 , show source phrase Table 2 , generated sample RNN Encoder – Decoder . For source phrase , generated 50 sample show top - ﬁve phrase accordingly scores . We see RNN Encoder – Decoder able propose well - formed target phrase without looking actual phrase table . Importantly , generated phrase overlap completely target phrase phrase table . This encourages u investigate possibility replacing whole part phrase table Figure 5 : 2 – D embedding learned phrase representation . The top left one show full representation space , three ﬁgures show zoomed - in view speciﬁc region . proposed RNN Encoder – Decoder future . 4 . 4 Word Phrase Representations Since proposed RNN Encoder – Decoder speciﬁcally designed task machine translation , brieﬂy look property trained model . ). Since proposed RNN Encoder – Decoder also project map back sequence word continuous space vector , expect see similar property proposed model well . The left plot Fig . 4 show 2 – D embedding word using word embedding matrix learned RNN Encoder – Decoder . The projection done recently proposed BarnesHut - SNE . We clearly see semantically similar word clustered . The proposed RNN Encoder – Decoder naturally generates continuous - space representation phrase . The representation case 1000 - dimensional vector . Similarly word representations , visualize representation phrase consists four word using Barnes - Hut - SNE Fig . 5 . From visualization , clear RNN Encoder – Decoder capture semantic syntactic structure phrases . For instance , bottom - left plot , phrase duration time , phrase syntactically similar clustered together . The bottom - right plot show cluster phrase semantically similar . On hand , top - right plot show phrase syntactically similar . 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "blob_object = TextBlob(text_str)\n",
        "sent = list(blob_object.sentences)\n",
        "print(sent)\n",
        "print(len(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkWFL5rzKT-j",
        "outputId": "120cf9c2-3d26-474d-df97-171f707674d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sentence(\"Abstract In paper , propose novel neural network model called RNN Encoder Decoder consists two recurrent neural network .\"), Sentence(\"One RNN encodes sequence symbol ﬁxedlength vector representation , decodes representation another sequence symbols .\"), Sentence(\"The encoder decoder proposed model jointly trained maximize conditional probability target sequence given source sequence .\"), Sentence(\"The performance statistical machine translation system empirically found improve using conditional probability phrase pair computed RNN Encoder – Decoder additional feature existing log - linear model .\"), Sentence(\"Qualitatively , show proposed model learns semantically syntactically meaningful representation linguistic phrases .\"), Sentence(\"1 Introduction Deep neural network shown great success various application objection recognition ) speech recognition ).\"), Sentence(\"Furthermore , many recent work showed neural network successfully used number task natural language processing .\"), Sentence(\"These include , limited to , language modeling , paraphrase detection word embedding extraction .\"), Sentence(\"In ﬁeld statistical machine translation , deep neural network begun show promising results .\"), Sentence(\"summarizes successful usage feedforward neural network framework phrase - based SMT system .\"), Sentence(\"Along line research using neural network SMT , paper focus novel neural network architecture used part conventional phrase - based SMT system .\"), Sentence(\"The proposed neural network architecture , refer RNN Encoder – Decoder , consists two recurrent neural network act encoder decoder pair .\"), Sentence(\"The encoder map variable - length source sequence ﬁxed - length vector , decoder map vector representation back variable - length target sequence .\"), Sentence(\"The two network trained jointly maximize conditional probability target sequence given source sequence .\"), Sentence(\"Additionally , propose use rather sophisticated hidden unit order improve memory capacity ease training .\"), Sentence(\"The proposed RNN Encoder – Decoder novel hidden unit empirically evaluated task translating English French .\"), Sentence(\"We train model learn translation probability English phrase corresponding French phrase .\"), Sentence(\"The model used part standard phrase - based SMT system scoring phrase pair phrase table .\"), Sentence(\"The empirical evaluation reveals approach scoring phrase pair RNN Encoder – Decoder improves translation performance .\"), Sentence(\"We qualitatively analyze trained RNN Encoder – Decoder comparing phrase score given existing translation model .\"), Sentence(\"The qualitative analysis show RNN Encoder – Decoder better capturing linguistic regularity phrase table , indirectly explaining quantitative improvement overall translation performance .\"), Sentence(\"The analysis model reveals RNN Encoder Decoder learns continuous space representation phrase preserve semantic syntactic structure phrase .\"), Sentence(\"4 1 0 2 p e S 3 ] L C .\"), Sentence(\"c neural network consists hidden state h optional output operates variablelength sequence x = .\"), Sentence(\"At time step t , hidden state RNN updated = f − 1 , , non - linear f activation funcf may simple elementtion .\"), Sentence(\"wise logistic sigmoid function complex long short - term memory unit .\"), Sentence(\"An RNN learn probability distribution sequence trained predict next symbol sequence .\"), Sentence(\"In case , output timestep conditional distribution p .\"), Sentence(\"For example , multinomial distribution output using softmax activation function p = K exp wjht j = 1 exp wjht possible symbol j = 1 , .\"), Sentence(\".\"), Sentence(\".\"), Sentence(\", K , row weight matrix W .\"), Sentence(\"By combining probabilities , compute probability sequence x using Figure 1 : An illustration proposed RNN Encoder – Decoder .\"), Sentence(\"note input output sequence length T T may differ .\"), Sentence(\"The encoder RNN read symbol input sequence x sequentially .\"), Sentence(\"As read symbol , hidden state RNN change according .\"), Sentence(\".\"), Sentence(\"After reading end sequence , hidden state RNN summary c whole input sequence .\"), Sentence(\"The decoder proposed model another RNN trained generate output sequence predicting next symbol given hidden state .\"), Sentence(\"However , unlike RNN described Sec .\"), Sentence(\"2 .\"), Sentence(\"1 , also conditioned − 1 summary c input sequence .\"), Sentence(\"Hence , hidden state decoder time computed by , , p = T t = 1 p .\"), Sentence(\"= f − 1 , − 1 , c , From learned distribution , straightforward sample new sequence iteratively sampling symbol time step .\"), Sentence(\"2 .\"), Sentence(\"2 RNN Encoder – Decoder In paper , propose novel neural network architecture learns encode variable - length sequence ﬁxed - length vector representation decode given ﬁxed - length vector representation back variable - length sequence .\"), Sentence(\"From probabilistic perspective , new model general method learn conditional distribution variable - length sequence conditioned yet another variable - length sequence , | x1 , .\"), Sentence(\".\"), Sentence(\".\"), Sentence(\", ), one e .\"), Sentence(\"g .\"), Sentence(\"p similarly , conditional distribution next symbol P = g , − 1 , c .\"), Sentence(\"given activation function f g .\"), Sentence(\"See Fig .\"), Sentence(\"1 graphical depiction pro posed model architecture .\"), Sentence(\"The two component proposed RNN Encoder – Decoder jointly trained maximize conditional log - likelihood max 1 N N n = 1 log , x1x2xTyT ' y2y1cDecoderEncoder set model parameter pair training set .\"), Sentence(\"In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .\"), Sentence(\"Once RNN Encoder – Decoder trained , model used two ways .\"), Sentence(\"One way use model generate target sequence given input sequence .\"), Sentence(\"On hand , model used score given pair input output sequences , score simply probability Eqs .\"), Sentence(\".\"), Sentence(\"2 .\"), Sentence(\"3 Hidden Unit Adaptively Remembers Forgets In addition novel model architecture , also propose new type hidden unit ) motivated LSTM unit much simpler compute implement .\"), Sentence(\"1 Fig .\"), Sentence(\"2 show graphical depiction proposed hidden unit .\"), Sentence(\"Let u describe activation j - th hidden unit computed .\"), Sentence(\"First , reset gate computed = j + Urht − 1 , j logistic sigmoid function , j denotes j - th element vector .\"), Sentence(\"x − 1 input previous hidden state , respectively .\"), Sentence(\"Ur weight matrix learned .\"), Sentence(\"Similarly , update gate computed = j + Uzht − 1 .\"), Sentence(\"j The actual activation proposed unit computed j = zjht − 1 j + ˜ j , ˜ j = j + U r − 1 .\"), Sentence(\"j In formulation , reset gate close 0 , hidden state forced ignore previous hidden state reset current input 1 The LSTM unit , shown impressive result several application speech recognition , memory cell four gating unit adaptively control information ﬂow inside unit , compared two gating unit proposed hidden unit .\"), Sentence(\"For detail LSTM networks , see , e .\"), Sentence(\"g ., .\"), Sentence(\"Figure 2 : An illustration proposed hidden activation function .\"), Sentence(\"The update gate z selects whether hidden state updated new hidden state ˜ h .\"), Sentence(\"The reset gate r decides whether previous hidden state ignored .\"), Sentence(\"See Eqs .\"), Sentence(\"detailed equation r , z , h ˜ h .\"), Sentence(\"only .\"), Sentence(\"This effectively allows hidden state drop information found irrelevant later future , thus , allowing compact representation .\"), Sentence(\"On hand , update gate control much information previous hidden state carry current hidden state .\"), Sentence(\"This act similarly memory cell LSTM network help RNN remember longterm information .\"), Sentence(\"Furthermore , may considered adaptive variant leaky - integration unit .\"), Sentence(\"As hidden unit separate reset update gates , hidden unit learn capture dependency different time scales .\"), Sentence(\"Those unit learn capture short - term dependency tend reset gate frequently active , capture longer - term dependency update gate mostly active .\"), Sentence(\"In preliminary experiments , found crucial use new unit gating units .\"), Sentence(\"We able get meaningful result oft - used tanh unit without gating .\"), Sentence(\"3 Statistical Machine Translation In commonly used statistical machine translation system , goal system ﬁnd translation f given source sentence e , maximizes p ∝ , ﬁrst term right hand side called translation model latter language model In practice , however , ).\"), Sentence(\"SMT system model log p loglinear model additional feature corre zrhh ~ x sponding weights : log p = N n = 1 wnfn + log Z , In phrase - based n - th feature weight , respectively .\"), Sentence(\"Z normalization constant depend weights .\"), Sentence(\"The weight often optimized maximize BLEU score development set .\"), Sentence(\"framework introduced translation model , log p factorized translation probability matching phrase source target sentences .\"), Sentence(\"2 These probability considered additional feature log - linear model ) weighted accordingly maximize BLEU score .\"), Sentence(\"SMT Since neural net language model proposed , neural network used widely SMT systems .\"), Sentence(\"In many cases , neural network used rescore translation hypothesis ).\"), Sentence(\"Recently , however , interest training neural network score translated sentence using representation source sentence additional input .\"), Sentence(\"See , e .\"), Sentence(\"g ., , .\"), Sentence(\"3 .\"), Sentence(\"1 Scoring Phrase Pairs RNN Encoder – Decoder Here propose train RNN Encoder Decoder table phrase pair use score additional feature loglinear model .\"), Sentence(\"tuning SMT decoder .\"), Sentence(\"When train RNN Encoder – Decoder , ignore frequency phrase pair original corpora .\"), Sentence(\"This measure taken order reduce computational expense randomly selecting phrase pair large phrase table according normalized frequency ensure RNN Encoder Decoder simply learn rank phrase pair according number occurrences .\"), Sentence(\"One underlying reason choice existing translation probability phrase table already reﬂects frequency phrase 2 Without loss generality , on , refer p phrase pair translation model well pair original corpus .\"), Sentence(\"With ﬁxed capacity RNN Encoder – Decoder , try ensure capacity model focused toward learning linguistic regularities , i .\"), Sentence(\"e ., distinguishing plausible implausible translations , learning “ manifold ” plausible translations .\"), Sentence(\"Once RNN Encoder – Decoder trained , add new score phrase pair existing phrase table .\"), Sentence(\"This allows new score enter existing tuning algorithm minimal additional overhead computation .\"), Sentence(\"As Schwenk pointed , possible completely replace existing phrase table proposed RNN Encoder Decoder .\"), Sentence(\"In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .\"), Sentence(\"This requires , however , expensive sampling procedure perIn paper , thus , formed repeatedly .\"), Sentence(\"consider rescoring phrase pair phrase table .\"), Sentence(\"3 .\"), Sentence(\"2 Related Approaches : Neural Networks Machine Translation Before presenting empirical results , discus number recent work proposed use neural network context SMT .\"), Sentence(\"Schwenk proposed similar approach scoring phrase pairs .\"), Sentence(\"Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .\"), Sentence(\"When used speciﬁcally scoring phrase SMT system , maximum phrase length often chosen small .\"), Sentence(\"However , length phrase increase apply neural network variable - length sequence data , important neural network handle variable - length input output .\"), Sentence(\"The proposed RNN Encoder – Decoder well - suited applications .\"), Sentence(\"Similar , Devlin al .\"), Sentence(\"proposed use feedforward neural network model translation model , however , predicting one word target phrase time .\"), Sentence(\"They reported impressive improvement , approach still requires maximum length input phrase ﬁxed priori .\"), Sentence(\"Although exactly neural network train , author proposed learn bilingual embedding words / phrases .\"), Sentence(\"They use learned embedding compute distance pair phrase used additional score phrase pair SMT system .\"), Sentence(\"In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .\"), Sentence(\"This closely related proposed RNN Encoder – Decoder model proposed , except input representation phrase bag - of - words .\"), Sentence(\"A similar approach using bag - of - words representation proposed well .\"), Sentence(\"Earlier , similar encoder – decoder model using two recursive neural network proposed , model restricted monolingual setting , i .\"), Sentence(\"e .\"), Sentence(\"model reconstructs input sentence .\"), Sentence(\"More recently , another encoder – decoder model using RNN proposed , decoder conditioned representation either source sentence source context .\"), Sentence(\"One important difference proposed RNN Encoder – Decoder approach order word source target phrase taken account .\"), Sentence(\"The RNN Encoder – Decoder naturally distinguishes sequence word different order , whereas aforementioned approach effectively ignore order information .\"), Sentence(\"The closest approach related proposed RNN Encoder – Decoder Recurrent Continuous Translation Model proposed In pa .\"), Sentence(\"per , proposed similar model consists encoder decoder .\"), Sentence(\"The difference model used convolutional n - gram model encoder hybrid inverse CGM recurrent neural network decoder .\"), Sentence(\"They , however , evaluated model rescoring n - best list proposed conventional SMT system computing perplexity gold standard translations .\"), Sentence(\"4 Experiments We evaluate approach English / French translation task WMT ’ 14 workshop .\"), Sentence(\"4 .\"), Sentence(\"1 Data Baseline System Large amount resource available build English / French SMT system framework WMT ’ 14 translation task .\"), Sentence(\"The bilingual corpus include Europarl , news commentary , UN , two crawled corpus 90M 780M word respectively .\"), Sentence(\"The last two corpus quite noisy .\"), Sentence(\"To train French language model , 712M word crawled newspaper material available addition target side bitexts .\"), Sentence(\"All word count refer French word tokenization .\"), Sentence(\"It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .\"), Sentence(\"Instead , one focus relevant subset data given task .\"), Sentence(\"We done applying data selection method proposed , extension bitexts .\"), Sentence(\"By mean selected subset 418M word 2G word language modeling subset 348M 850M word training RNN Encoder – Decoder .\"), Sentence(\"We used test set newstest2012 2013 data selection weight tuning MERT , newstest2014 test set .\"), Sentence(\"Each set 70 thousand word single reference translation .\"), Sentence(\"For training neural networks , including proposed RNN Encoder – Decoder , limited source target vocabulary frequent 15 , 000 word English French .\"), Sentence(\"This cover approximately 93 % dataset .\"), Sentence(\"All out - of - vocabulary word mapped special token ).\"), Sentence(\"The baseline phrase - based SMT system built using Moses default settings .\"), Sentence(\"This system achieves BLEU score 30 .\"), Sentence(\"64 33 .\"), Sentence(\"3 development test sets , respectively .\"), Sentence(\"4 .\"), Sentence(\"1 .\"), Sentence(\"1 RNN Encoder – Decoder The RNN Encoder – Decoder used experiment 1000 hidden unit proposed gate encoder decoder .\"), Sentence(\"The input matrix input symbol hidden unit approximated two lower - rank matrices , output matrix approximated BLEU tem add redundant .\"), Sentence(\"Models dev 30 .\"), Sentence(\"64 Baseline 31 .\"), Sentence(\"20 RNN 31 .\"), Sentence(\"48 CSLM + RNN CSLM + RNN + 31 .\"), Sentence(\"50 test 33 .\"), Sentence(\"30 33 .\"), Sentence(\"87 34 .\"), Sentence(\"64 34 .\"), Sentence(\"54 Table 1 : BLEU score computed development test set using different combination approaches .\"), Sentence(\"denotes word penalty , penalizes number unknown word neural networks .\"), Sentence(\"similarly .\"), Sentence(\"We used rank - 100 matrices , equivalent learning embedding dimension 100 word .\"), Sentence(\"The activation function used ˜ h .\"), Sentence(\"hyperbolic tangent function .\"), Sentence(\"The computation hidden state decoder output implemented deep neural network single intermediate layer 500 maxout unit pooling 2 input .\"), Sentence(\"All weight parameter RNN Encoder Decoder initialized sampling isotropic zero - mean Gaussian distribution standard deviation ﬁxed 0 .\"), Sentence(\"01 , except recurrent weight parameters .\"), Sentence(\"For recurrent weight matrices , ﬁrst sampled white Gaussian distribution used left singular vector matrix , following .\"), Sentence(\"We used Adadelta stochastic gradient descent train RNN Encoder – Decoder hyperparameters = 10 − 6 = 0 .\"), Sentence(\"95 .\"), Sentence(\"At update , used 64 randomly selected phrase pair phrase table .\"), Sentence(\"The model trained approximately three days .\"), Sentence(\"Details architecture used experiment explained depth supplementary material .\"), Sentence(\"4 .\"), Sentence(\"1 .\"), Sentence(\"2 Neural Language Model In order ass effectiveness scoring phrase pair proposed RNN Encoder Decoder , also tried traditional approach using neural network learning target language model .\"), Sentence(\"Especially , comparison SMT system using CSLM using proposed approach phrase scoring RNN Encoder – Decoder clarify whether contribution multiple neural network different part SMT sys We trained CSLM model 7 - grams target corpus .\"), Sentence(\"Each input word projected embedding space R512 , concatenated form 3072dimensional vector .\"), Sentence(\"The concatenated vector fed two rectiﬁed layer .\"), Sentence(\"The output layer simple softmax layer ).\"), Sentence(\"All weight parameter initialized uniformly − 0 .\"), Sentence(\"01 0 .\"), Sentence(\"01 , model trained validation perplexity improve 10 epochs .\"), Sentence(\"After training , language model achieved perplexity 45 .\"), Sentence(\"80 .\"), Sentence(\"The validation set random selection 0 .\"), Sentence(\"1 % corpus .\"), Sentence(\"The model used score partial translation decoding process , generally lead higher gain BLEU score n - best list rescoring .\"), Sentence(\"To address computational complexity using CSLM decoder buffer used aggregate n - grams stacksearch performed decoder .\"), Sentence(\"Only full , stack buffer pruned , n - grams scored CSLM .\"), Sentence(\"This allows u perform fast matrixmatrix multiplication GPU using Theano .\"), Sentence(\"Figure 3 : The visualization phrase pair according score RNN Encoder – Decoder translation model .\"), Sentence(\"4 .\"), Sentence(\"2 Quantitative Analysis We tried following combinations : 1 .\"), Sentence(\"Baseline conﬁguration 2 .\"), Sentence(\"Baseline + RNN 3 .\"), Sentence(\"Baseline + CSLM + RNN 4 .\"), Sentence(\"Baseline + CSLM + RNN + Word penalty − 60 − 50 − 40 − 30 − 20 − 100 − 14 − 12 − 10 − 8 − 6 − 4 − 20RNN Scores Scores Source end ﬁrst time United States , well one Translation Model pour la premir ¨ ere fois ] RNN Encoder – Decoder Long , frequent source phrase Source , Minister Communications Transport comply part world .\"), Sentence(\"past day .\"), Sentence(\"Friday Saturday Translation Model gions monde .]\"), Sentence(\"RNN Encoder – Decoder Long , rare source phrase Table 2 : The top scoring target phrase small set source phrase according translation model RNN Encoder – Decoder .\"), Sentence(\"Source phrase randomly selected phrase 4 words .\"), Sentence(\"?\"), Sentence(\"denotes incomplete character .\"), Sentence(\"r Cyrillic letter ghe .\"), Sentence(\"The result presented Table 1 .\"), Sentence(\"As expected , adding feature computed neural network consistently improves performance baseline performance .\"), Sentence(\"The best performance achieved used CSLM phrase score RNN Encoder – Decoder .\"), Sentence(\"This suggests contribution CSLM RNN Encoder Decoder correlated one expect better result improving method independently .\"), Sentence(\"Furthermore , tried penalizing number word unknown neural network .\"), Sentence(\"We simply adding number unknown word additional feature loglinear model .\"), Sentence(\".\"), Sentence(\"3 However , case 3 To understand effect penalty , consider set word 15 , 000 large shortlist , .\"), Sentence(\"All word xi /∈ replaced special token scored neural networks .\"), Sentence(\"Hence , conditional probability xi /∈ actually given model p | x < t ) = p = p | x < t ≥ p xi | x < t , /∈ x < t shorthand notation − 1 , .\"), Sentence(\".\"), Sentence(\".\"), Sentence(\", x1 .\"), Sentence(\"able achieve better performance test set , development set .\"), Sentence(\"4 .\"), Sentence(\"3 Qualitative Analysis In order understand performance improvement come from , analyze phrase pair score computed RNN Encoder – Decoder corresponding p translation model .\"), Sentence(\"Since existing translation model relies solely statistic phrase pair corpus , expect score better estimated frequent phrase badly estimated rare phrases .\"), Sentence(\"Also , mentioned earlier Sec .\"), Sentence(\"3 .\"), Sentence(\"1 , expect RNN Encoder Decoder trained without frequency information score phrase pair based rather linguistic regularity statistic occurrence corpus .\"), Sentence(\"We focus pair whose source phrase long As result , probability word shortlist always overestimated .\"), Sentence(\"It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .\"), Sentence(\"Source end ﬁrst time United States , well one Samples RNN Encoder – Decoder Long , frequent source phrase Source , Minister Communications Transport comply part world .\"), Sentence(\"past day .\"), Sentence(\"Friday Saturday Samples RNN Encoder – Decoder Long , rare source phrase Table 3 : Samples generated RNN Encoder – Decoder source phrase used Table 2 .\"), Sentence(\"We show top - 5 target phrase 50 samples .\"), Sentence(\"They sorted RNN Encoder – Decoder scores .\"), Sentence(\"Figure 4 : 2 – D embedding learned word representation .\"), Sentence(\"The left one show full embedding space , right one show zoomed - in view one region .\"), Sentence(\"For plots , see supplementary material .\"), Sentence(\"frequent .\"), Sentence(\"For source phrase , look target phrase scored high either translation probability p RNN Encoder – Decoder .\"), Sentence(\"Similarly , perform procedure pair whose source phrase long rare corpus .\"), Sentence(\"Table 2 list top - 3 target phrase per source phrase favored either translation model RNN Encoder – Decoder .\"), Sentence(\"The source phrase randomly chosen among long one 4 5 words .\"), Sentence(\"In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .\"), Sentence(\"We observe RNN Encoder – Decoder prefers shorter phrase general .\"), Sentence(\"Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .\"), Sentence(\"This could arise proposed approach training RNN Encoder Decoder set unique phrase pairs , discouraging RNN Encoder – Decoder learning simply frequency phrase pair corpus , explained earlier .\"), Sentence(\"Furthermore , Table 3 , show source phrase Table 2 , generated sample RNN Encoder – Decoder .\"), Sentence(\"For source phrase , generated 50 sample show top - ﬁve phrase accordingly scores .\"), Sentence(\"We see RNN Encoder – Decoder able propose well - formed target phrase without looking actual phrase table .\"), Sentence(\"Importantly , generated phrase overlap completely target phrase phrase table .\"), Sentence(\"This encourages u investigate possibility replacing whole part phrase table Figure 5 : 2 – D embedding learned phrase representation .\"), Sentence(\"The top left one show full representation space , three ﬁgures show zoomed - in view speciﬁc region .\"), Sentence(\"proposed RNN Encoder – Decoder future .\"), Sentence(\"4 .\"), Sentence(\"4 Word Phrase Representations Since proposed RNN Encoder – Decoder speciﬁcally designed task machine translation , brieﬂy look property trained model .\"), Sentence(\").\"), Sentence(\"Since proposed RNN Encoder – Decoder also project map back sequence word continuous space vector , expect see similar property proposed model well .\"), Sentence(\"The left plot Fig .\"), Sentence(\"4 show 2 – D embedding word using word embedding matrix learned RNN Encoder – Decoder .\"), Sentence(\"The projection done recently proposed BarnesHut - SNE .\"), Sentence(\"We clearly see semantically similar word clustered .\"), Sentence(\"The proposed RNN Encoder – Decoder naturally generates continuous - space representation phrase .\"), Sentence(\"The representation case 1000 - dimensional vector .\"), Sentence(\"Similarly word representations , visualize representation phrase consists four word using Barnes - Hut - SNE Fig .\"), Sentence(\"5 .\"), Sentence(\"From visualization , clear RNN Encoder – Decoder capture semantic syntactic structure phrases .\"), Sentence(\"For instance , bottom - left plot , phrase duration time , phrase syntactically similar clustered together .\"), Sentence(\"The bottom - right plot show cluster phrase semantically similar .\"), Sentence(\"On hand , top - right plot show phrase syntactically similar .\"), Sentence(\"5\")]\n",
            "280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n",
        "\n",
        "\n",
        "def _replace_whitespace(match):\n",
        "    text = match.group()\n",
        "    if \"\\n\" in text or \"\\r\" in text:\n",
        "        return \"\\n\"\n",
        "    else:\n",
        "        return \" \"\n",
        "\n",
        "\n",
        "def is_blank(string):\n",
        "    return not string or string.isspace()\n",
        "\n",
        "\n",
        "def get_symmetric_matrix(matrix):\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "\n",
        "def core_cosine_similarity(vector1, vector2):\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "\n",
        "class TextRank4Sentences():\n",
        "    def __init__(self):\n",
        "        self.damping = 0.85 \n",
        "        self.min_diff = 1e-5\n",
        "        self.steps = 100 \n",
        "        self.text_str = None\n",
        "        self.sentences = None\n",
        "        self.pr_vector = None\n",
        "\n",
        "    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n",
        "        if stopwords is None:\n",
        "            stopwords = []\n",
        "\n",
        "        sent1 = [w.lower() for w in sent1]\n",
        "        sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "        all_words = list(set(sent1 + sent2))\n",
        "\n",
        "        vector1 = [0] * len(all_words)\n",
        "        vector2 = [0] * len(all_words)\n",
        "\n",
        "        for w in sent1:\n",
        "            if w in stopwords:\n",
        "                continue\n",
        "            vector1[all_words.index(w)] += 1\n",
        "\n",
        "        for w in sent2:\n",
        "            if w in stopwords:\n",
        "                continue\n",
        "            vector2[all_words.index(w)] += 1\n",
        "\n",
        "        return core_cosine_similarity(vector1, vector2)\n",
        "\n",
        "    def _build_similarity_matrix(self, sentences, stopwords=None):\n",
        "        sm = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1 == idx2:\n",
        "                    continue\n",
        "\n",
        "                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n",
        "\n",
        "        sm = get_symmetric_matrix(sm)\n",
        "\n",
        "        norm = np.sum(sm, axis=0)\n",
        "        sm_norm = np.divide(sm, norm, where=norm != 0)\n",
        "\n",
        "        return sm_norm\n",
        "\n",
        "    def _run_page_rank(self, similarity_matrix):\n",
        "\n",
        "        pr_vector = np.array([1] * len(similarity_matrix))\n",
        "\n",
        "        previous_pr = 0\n",
        "        for epoch in range(self.steps):\n",
        "            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n",
        "            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n",
        "                break\n",
        "            else:\n",
        "                previous_pr = sum(pr_vector)\n",
        "\n",
        "        return pr_vector\n",
        "\n",
        "    def _get_sentence(self, index):\n",
        "\n",
        "        try:\n",
        "            return self.sentences[index]\n",
        "        except IndexError:\n",
        "            return \"\"\n",
        "\n",
        "    def get_top_sentences(self, number=5):\n",
        "\n",
        "        top_sentences = []\n",
        "\n",
        "        if self.pr_vector is not None:\n",
        "\n",
        "            sorted_pr = np.argsort(self.pr_vector)\n",
        "            sorted_pr = list(sorted_pr)\n",
        "            sorted_pr.reverse()\n",
        "\n",
        "            index = 0\n",
        "            for epoch in range(number):\n",
        "                sent = self.sentences[sorted_pr[index]]\n",
        "                sent = normalize_whitespace(sent)\n",
        "                top_sentences.append(sent)\n",
        "                index += 1\n",
        "\n",
        "        return top_sentences\n",
        "\n",
        "    def analyze(self, text, stop_words=None):\n",
        "        self.text_str = text\n",
        "        self.sentences = sent_tokenize(self.text_str)\n",
        "\n",
        "        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n",
        "\n",
        "        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n",
        "\n",
        "        self.pr_vector = self._run_page_rank(similarity_matrix)\n",
        "\n",
        "\n",
        "tr4sh = TextRank4Sentences()\n",
        "tr4sh.analyze(text_str)\n",
        "print(len(sent))\n",
        "sentences = tr4sh.get_top_sentences(len(sent))\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfv55-b0Hyba",
        "outputId": "9642b2c3-64ef-4a40-8a19-c557bd2f66f4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280\n",
            "280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = 1\n",
        "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "use_cuda = True\n",
        "model = model.cuda() if use_cuda else model\n",
        "W2V_PATH = 'GloVe/glove.840B.300d.txt' if model_version == 1 else 'fastText/crawl-300d-2M.vec'\n",
        "model.set_w2v_path(W2V_PATH)\n",
        "model.build_vocab_k_words(K=100000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zpaxpcLJqjo",
        "outputId": "5156c822-e16e-4f78-b230-79bf12746da6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size : 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imp = []\n",
        "pow = [\"accordingly\", \"furthermore\", \"moreover\", \"similarly\", \"also\", \"hence\", \"namely\", \"still\", \"anyway\", \"however\", \"nevertheless\", \"then\", \"besides\", \"incidentally\", \"next\", \"thereafter\", \"certainly\", \"indeed\", \"nonetheless\", \"therefore\", \"consequently\", \"instead\", \"now\", \"thus\", \"finally\", \"likewise\", \"otherwise\", \"undoubtedly\", \"further\", \"meanwhile\"]\n",
        "\n",
        "for i in pow:\n",
        "  for j in sentences:\n",
        "    if i in j:\n",
        "      imp.append(j)"
      ],
      "metadata": {
        "id": "IRKbN5CTha0y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imp)\n",
        "print(len(imp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4QhYZqp2MwW",
        "outputId": "1707f1c3-3fd1-4c57-e53a-480b6e9ff9db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['For source phrase , generated 50 sample show top - ﬁve phrase accordingly scores .', '2 These probability considered additional feature log - linear model ) weighted accordingly maximize BLEU score .', 'similarly .', 'Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .', 'p similarly , conditional distribution next symbol P = g , − 1 , c .', 'This act similarly memory cell LSTM network help RNN remember longterm information .', '2 Neural Language Model In order ass effectiveness scoring phrase pair proposed RNN Encoder Decoder , also tried traditional approach using neural network learning target language model .', 'Since proposed RNN Encoder – Decoder also project map back sequence word continuous space vector , expect see similar property proposed model well .', '1 , also conditioned − 1 summary c input sequence .', '3 Hidden Unit Adaptively Remembers Forgets In addition novel model architecture , also propose new type hidden unit ) motivated LSTM unit much simpler compute implement .', 'They reported impressive improvement , approach still requires maximum length input phrase ﬁxed priori .', 'proposed use feedforward neural network model translation model , however , predicting one word target phrase time .', 'This requires , however , expensive sampling procedure perIn paper , thus , formed repeatedly .', 'They , however , evaluated model rescoring n - best list proposed conventional SMT system computing perplexity gold standard translations .', 'Recently , however , interest training neural network score translated sentence using representation source sentence additional input .', 'It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .', '3 Statistical Machine Translation In commonly used statistical machine translation system , goal system ﬁnd translation f given source sentence e , maximizes p ∝ , ﬁrst term right hand side called translation model latter language model In practice , however , ).', 'p similarly , conditional distribution next symbol P = g , − 1 , c .', 'The decoder proposed model another RNN trained generate output sequence predicting next symbol given hidden state .', 'An RNN learn probability distribution sequence trained predict next symbol sequence .', 'It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .', 'Furthermore , tried penalizing number word unknown neural network .', 'denotes word penalty , penalizes number unknown word neural networks .', 'We simply adding number unknown word additional feature loglinear model .', 'It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .', 'This requires , however , expensive sampling procedure perIn paper , thus , formed repeatedly .', 'This effectively allows hidden state drop information found irrelevant later future , thus , allowing compact representation .']\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(imp, bsize=128, tokenize=False, verbose=True)\n",
        "print('nb sentences encoded : {0}'.format(len(embeddings)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoHP-UK6zg4Q",
        "outputId": "de9caab2-7d16-40ea-c5d4-c1b75941662e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb words kept : 517/548 (94.3%)\n",
            "Speed : 211.5 sentences/s (gpu mode, bsize=128)\n",
            "nb sentences encoded : 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "metadata": {
        "id": "NFgug6S1zunS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_problem = {}\n",
        "research_kw =[]\n",
        "research_stmt = []\n",
        "for i in sentences:\n",
        "  research_problem[i] = round(cosine(model.encode(i)[0], model.encode(['research problem issue'])[0]), 2)\n",
        "\n",
        "sorted_x = dict(sorted(research_problem.items(), key=lambda kv: kv[1]))\n",
        "for i, j in sorted_x.items():\n",
        "  if j > list(set(sorted_x.values()))[-3]:\n",
        "    rake = Rake()\n",
        "    keywords = rake.apply(i)\n",
        "    research_kw.append(keywords[:3])\n",
        "    research_stmt.append(i)\n",
        "\n",
        "print(research_kw)\n",
        "print(research_stmt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4nApAYD-HHe",
        "outputId": "821dd1b1-9aaa-45c4-95a7-b2614d9b665c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], [('source phrase', 4.0), ('rnn encoder', 4.0), ('case', 1.0)], [('cases', 1.0)], [('output decoder', 4.0), ('starting input', 4.0), ('case', 1.0)], [('interestingly', 1.0), ('decoder', 1.0)], [], [('case', 1.0)], [('importantly', 1.0)], [('rescore translation hypothesis', 9.0), ('neural network', 4.0), ('cases', 1.0)], [('task', 1.0)], [('shortlisted word', 4.0), ('paper', 1.0)], [], [('unit gating units', 9.0), ('preliminary experiments', 4.0), ('found crucial', 4.0)], [('based neural network', 9.0), ('size input ﬁxed', 8.5), ('size output', 4.5)], []]\n",
            "['?', 'In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .', 'In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .', 'In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .', 'Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .', 'In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .', 'In case , output timestep conditional distribution p .', 'Importantly , generated phrase overlap completely target phrase phrase table .', 'In many cases , neural network used rescore translation hypothesis ).', 'Instead , one focus relevant subset data given task .', 'It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .', 'In ﬁeld statistical machine translation , deep neural network begun show promising results .', 'In preliminary experiments , found crucial use new unit gating units .', 'Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .', 'It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apprch = {}\n",
        "apprch_kw = []\n",
        "approach_stmt = []\n",
        "for i in sentences:\n",
        "  apprch[i] = round(cosine(model.encode(i)[0], model.encode(['used approach tasks steps algorithm method procedure technique process style'])[0]), 2)\n",
        "\n",
        "sorted_x = dict(sorted(apprch.items(), key=lambda kv: kv[1]))\n",
        "for i, j in sorted_x.items():\n",
        "  if j > list(set(sorted_x.values()))[-3]:\n",
        "        rake = Rake()\n",
        "        keywords = rake.apply(i)\n",
        "        apprch_kw.append(keywords[:2])\n",
        "        approach_stmt.append(i)\n",
        "\n",
        "print(apprch_kw)\n",
        "print(approach_stmt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-2hHds8-0tg",
        "outputId": "c89993e3-2d55-49c9-ea64-34ca5046e033"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('01 0', 0)], [('recurrent weight parameters', 9.0)], [], [('learned distribution', 4.0), ('straightforward sample', 4.0)], []]\n",
            "['01 0 .', '01 , except recurrent weight parameters .', '01 , model trained validation perplexity improve 10 epochs .', '= f − 1 , − 1 , c , From learned distribution , straightforward sample new sequence iteratively sampling symbol time step .', 'x − 1 input previous hidden state , respectively .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expt = {}\n",
        "expt_kw = []\n",
        "expt_stmt = []\n",
        "for i in sentences:\n",
        "  expt[i] = round(cosine(model.encode(i)[0], model.encode([\"experimental setup test investigation examination experimentation testing\"])[0]), 2)\n",
        "\n",
        "sorted_x = dict(sorted(expt.items(), key=lambda kv: kv[1]))\n",
        "for i, j in sorted_x.items():\n",
        "  if j > list(set(sorted_x.values()))[-3]:\n",
        "        rake = Rake()\n",
        "        keywords = rake.apply(i)\n",
        "        expt_kw.append(keywords[:2])\n",
        "        expt_stmt.append(i)\n",
        "\n",
        "print(expt_kw)\n",
        "print(expt_stmt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6L0eHNF_liZ",
        "outputId": "056bfc1a-692e-49b0-ee69-af714e84365f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('01 0', 0)], [('recurrent weight parameters', 9.0)], [], [('source phrase', 4.0), ('rnn encoder', 4.0)], [('cases', 1.0)], [('output decoder', 4.0), ('starting input', 4.0)], [('interestingly', 1.0), ('decoder', 1.0)], [], [('case', 1.0)], [('importantly', 1.0)], [('rescore translation hypothesis', 9.0), ('neural network', 4.0)], [('task', 1.0)], [('shortlisted word', 4.0), ('paper', 1.0)], [], [('unit gating units', 9.0), ('preliminary experiments', 4.0)], [('based neural network', 9.0), ('size input ﬁxed', 8.5)], []]\n",
            "['01 0 .', '01 , except recurrent weight parameters .', '01 , model trained validation perplexity improve 10 epochs .', 'In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .', 'In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .', 'In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .', 'Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .', 'In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .', 'In case , output timestep conditional distribution p .', 'Importantly , generated phrase overlap completely target phrase phrase table .', 'In many cases , neural network used rescore translation hypothesis ).', 'Instead , one focus relevant subset data given task .', 'It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .', 'In ﬁeld statistical machine translation , deep neural network begun show promising results .', 'In preliminary experiments , found crucial use new unit gating units .', 'Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .', 'It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = {}\n",
        "res_kw = []\n",
        "result_stmt = []\n",
        "for i in sentences:\n",
        "  res[i] = round(cosine(model.encode(i)[0], model.encode([\"result output consequence outcome conclusion product solution decision opinion findings answer solution\"])[0]), 2),\n",
        "\n",
        "sorted_x = dict(sorted(res.items(), key=lambda kv: kv[1]))\n",
        "for i, j in sorted_x.items():\n",
        "  if j > list(set(sorted_x.values()))[-3]:\n",
        "        rake = Rake()\n",
        "        keywords = rake.apply(i)\n",
        "        res_kw.append(keywords[:2])\n",
        "        result_stmt.append(i)\n",
        "\n",
        "print(res_kw)\n",
        "print(result_stmt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqe9h_wL_6Sx",
        "outputId": "54dcfc9c-606a-42b4-c8ec-39d7457ce172"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('source phrase', 4.0), ('rnn encoder', 4.0)], [('cases', 1.0)], [('output decoder', 4.0), ('starting input', 4.0)], [('interestingly', 1.0), ('decoder', 1.0)], [], [('case', 1.0)], [('importantly', 1.0)], [('rescore translation hypothesis', 9.0), ('neural network', 4.0)], [('task', 1.0)], [('shortlisted word', 4.0), ('paper', 1.0)], [], [('unit gating units', 9.0), ('preliminary experiments', 4.0)], [('based neural network', 9.0), ('size input ﬁxed', 8.5)], [], [('01 0', 0)], [('recurrent weight parameters', 9.0)], []]\n",
            "['In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .', 'In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .', 'In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .', 'Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .', 'In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .', 'In case , output timestep conditional distribution p .', 'Importantly , generated phrase overlap completely target phrase phrase table .', 'In many cases , neural network used rescore translation hypothesis ).', 'Instead , one focus relevant subset data given task .', 'It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .', 'In ﬁeld statistical machine translation , deep neural network begun show promising results .', 'In preliminary experiments , found crucial use new unit gating units .', 'Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .', 'It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .', '01 0 .', '01 , except recurrent weight parameters .', '01 , model trained validation perplexity improve 10 epochs .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tex = \"\".join(research_stmt) + \"\".join(approach_stmt) + \"\".join(expt_stmt) + \"\".join(result_stmt)\n",
        "print(tex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCRgKYRhuvjR",
        "outputId": "94dd712c-f93b-466c-9716-9200d47c046f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .In case , output timestep conditional distribution p .Importantly , generated phrase overlap completely target phrase phrase table .In many cases , neural network used rescore translation hypothesis ).Instead , one focus relevant subset data given task .It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .In ﬁeld statistical machine translation , deep neural network begun show promising results .In preliminary experiments , found crucial use new unit gating units .Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .01 0 .01 , except recurrent weight parameters .01 , model trained validation perplexity improve 10 epochs .= f − 1 , − 1 , c , From learned distribution , straightforward sample new sequence iteratively sampling symbol time step .x − 1 input previous hidden state , respectively .01 0 .01 , except recurrent weight parameters .01 , model trained validation perplexity improve 10 epochs .In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .In case , output timestep conditional distribution p .Importantly , generated phrase overlap completely target phrase phrase table .In many cases , neural network used rescore translation hypothesis ).Instead , one focus relevant subset data given task .It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .In ﬁeld statistical machine translation , deep neural network begun show promising results .In preliminary experiments , found crucial use new unit gating units .Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .In case , given source phrase , RNN Encoder – Decoder need generate list target phrases .In cases , choice target phrase RNN Encoder – Decoder closer actual literal translations .In case , output decoder , starting input , differentiable , use gradient - based algorithm estimate model parameters .Interestingly , many phrase pair scored similarly translation model RNN Encoder – Decoder , many phrase pair scored radically different .In , feedforward neural network trained learn mapping bag - of - words representation input phrase output phrase .In case , output timestep conditional distribution p .Importantly , generated phrase overlap completely target phrase phrase table .In many cases , neural network used rescore translation hypothesis ).Instead , one focus relevant subset data given task .It possible address issue backing existing model contain non - shortlisted word ) In paper , however , opt introducing word penalty instead , counteracts word probability overestimation .In ﬁeld statistical machine translation , deep neural network begun show promising results .In preliminary experiments , found crucial use new unit gating units .Instead RNN - based neural network , used feedforward neural network ﬁxed - size input ﬁxed - size output .It commonly acknowledged training statistical model concatenation data necessarily lead optimal performance , result extremely large model difﬁcult handle .01 0 .01 , except recurrent weight parameters .01 , model trained validation perplexity improve 10 epochs .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summary_text = summarizer(tex, max_length=int(len(tex.split(\" \"))*0.8), min_length=int(len(tex.split(\" \"))*0.4), do_sample=False)[0]['summary_text']\n",
        "print(summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJp3SUUPoka4",
        "outputId": "087370f9-9ca5-4e39-cd65-f5e4871f5507"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Many phrase pair scored similarly translation model RNN Encoder – Decoder – Decoder . In paper, however, opt introducing word penalty instead, counteracts word probability overestimation . In ﬁeld statistical machine machine machine translation , deep neural network begun show promising results . In study, opt introduces word penalty, instead of word penalty to counteract word probability overrearability . In research paper, opt to introduce word penalty for translation model . In report, many phrase pairs scored similar translation model, many phrases scored radically different . It is possible that the existing model contains non - shortlisted word (non - shortlisted word) In paper , however , opt to replace word penalty with word word for translation .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "abstract = \"In this paper, we propose a novel neu\u0002ral network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN en\u0002codes a sequence of symbols into a fixed\u0002length vector representation, and the other decodes the representation into another se\u0002quence of symbols. The encoder and de\u0002coder of the proposed model are jointly trained to maximize the conditional prob\u0002ability of a target sequence given a source sequence. The performance of a statisti\u0002cal machine translation system is empiri\u0002cally found to improve by using the con\u0002ditional probabilities of phrase pairs com\u0002puted by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.\" \n",
        "scores = scorer.score(abstract, summary_text)\n",
        "print(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ff_4UENSHLB",
        "outputId": "a0be1554-3f4e-42e7-d1f3-6855192afbec"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': Score(precision=0.23529411764705882, recall=0.39097744360902253, fmeasure=0.29378531073446323), 'rouge2': Score(precision=0.05454545454545454, recall=0.09090909090909091, fmeasure=0.06818181818181819), 'rougeL': Score(precision=0.12217194570135746, recall=0.20300751879699247, fmeasure=0.15254237288135591)}\n"
          ]
        }
      ]
    }
  ]
}